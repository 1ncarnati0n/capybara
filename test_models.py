#!/usr/bin/env python3
"""
ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸: LLM (iris-7b) vs Seq2Seq (NLLB)
ì½˜ì†” ì¶œë ¥ê³¼ ë™ì¼í•œ ë‚´ìš©ì„ capybara/ë¶„ì„ê²°ê³¼ ì— ë¡œê·¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import sys
import atexit
import time
from datetime import datetime
import torch
from vllm import LLM, SamplingParams
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ------------------------------------------------------------
# ê²°ê³¼ ë¡œê·¸: ì½˜ì†” ì¶œë ¥ê³¼ íŒŒì¼ ë™ì‹œ ê¸°ë¡ (ë¶„ì„ê²°ê³¼/)
# ------------------------------------------------------------
def _setup_result_logging(prefix: str = "test_models") -> str:
    base_dir = os.path.dirname(__file__)
    results_dir = os.path.join(base_dir, "ë¶„ì„ê²°ê³¼")
    os.makedirs(results_dir, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(results_dir, f"{prefix}_{ts}.log")

    fh = open(log_path, "w", encoding="utf-8")

    class Tee:
        def __init__(self, *streams):
            self.streams = streams
        def write(self, data):
            for s in self.streams:
                s.write(data)
            for s in self.streams:
                try:
                    s.flush()
                except Exception:
                    pass
        def flush(self):
            for s in self.streams:
                try:
                    s.flush()
                except Exception:
                    pass

    sys.stdout = Tee(sys.stdout, fh)
    sys.stderr = Tee(sys.stderr, fh)
    atexit.register(lambda: fh.close())
    print(f"[LOG] ê²°ê³¼ íŒŒì¼: {log_path}")
    return log_path

_setup_result_logging()

print("=" * 70)
print("ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸: LLM (iris-7b) vs Seq2Seq (NLLB)")
print("=" * 70)
print()

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ëŒ€í‘œì ì¸ ë¬¸ì¥ë“¤)
test_cases_en = [
    "Hello, how are you today?",
    "Thank you very much for your help.",
    "I hope you have a wonderful day ahead of you.",
    "The quick brown fox jumps over the lazy dog.",
    "Machine learning is transforming the world.",
]

test_cases_ko = [
    "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?",
    "ë„ì›€ì„ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤.",
    "ì•ìœ¼ë¡œ ë©‹ì§„ í•˜ë£¨ê°€ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.",
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
    "ê¸°ê³„ í•™ìŠµì€ ì„¸ìƒì„ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
]

# models í´ë” ìƒì„±
models_dir = os.path.join(os.getcwd(), "models")
os.makedirs(models_dir, exist_ok=True)

# ===== LLM ëª¨ë¸ í…ŒìŠ¤íŠ¸ =====
print("=" * 70)
print("1. LLM ëª¨ë¸ (iris-7b)")
print("=" * 70)
print()

print("ğŸ”„ LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
start_load = time.time()

llm = LLM(
    model="davidkim205/iris-7b",
    download_dir=models_dir,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.91,
    max_model_len=1024,
    dtype="auto",
    kv_cache_dtype="fp8",
    enforce_eager=True,
    trust_remote_code=True
)

# ê°œì„ ëœ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
sampling_params = SamplingParams(
    temperature=0.3,
    top_p=0.95,
    max_tokens=128,
    repetition_penalty=1.1,
    skip_special_tokens=True,
    stop=["\n", "English:", "Korean:", "---", "###"]
)

load_time_llm = time.time() - start_load
print(f"âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {load_time_llm:.2f}ì´ˆ)")
print()

# ì˜ì–´ â†’ í•œêµ­ì–´ (LLM)
print("ğŸ“ ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (LLM)")
prompts_en_ko = [f"""You are a professional translator. Translate the following English text to natural Korean. Preserve the tone and nuances.

Examples:
English: "Good morning."
Korean: "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤."

English: "Thank you very much for your help."
Korean: "ë„ì›€ì„ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤."

English: "I hope you have a wonderful day."
Korean: "ë©‹ì§„ í•˜ë£¨ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤."

Now translate this:
English: "{text}"
Korean:""" for text in test_cases_en]

start_translate = time.time()
outputs = llm.generate(prompts_en_ko, sampling_params)
translate_time_llm_en = time.time() - start_translate

llm_results_en = []
for i, (original, output) in enumerate(zip(test_cases_en, outputs), 1):
    translated = output.outputs[0].text.strip().strip('"').strip("'").strip()
    llm_results_en.append(translated)
    print(f"{i}. EN: {original}")
    print(f"   KO: {translated}")
    print()

print(f"â±ï¸  ë²ˆì—­ ì‹œê°„: {translate_time_llm_en:.2f}ì´ˆ")
print()

# í•œêµ­ì–´ â†’ ì˜ì–´ (LLM)
print("ğŸ“ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ (LLM)")
prompts_ko_en = [f"""You are a professional translator. Translate the following Korean text to natural English. Preserve the tone and nuances.

Examples:
Korean: "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤."
English: "Good morning."

Korean: "ë„ì›€ì„ ì£¼ì…”ì„œ ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤."
English: "Thank you very much for your help."

Korean: "ë©‹ì§„ í•˜ë£¨ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤."
English: "I hope you have a wonderful day."

Now translate this:
Korean: "{text}"
English:""" for text in test_cases_ko]

start_translate = time.time()
outputs = llm.generate(prompts_ko_en, sampling_params)
translate_time_llm_ko = time.time() - start_translate

llm_results_ko = []
for i, (original, output) in enumerate(zip(test_cases_ko, outputs), 1):
    translated = output.outputs[0].text.strip().strip('"').strip("'").strip()
    llm_results_ko.append(translated)
    print(f"{i}. KO: {original}")
    print(f"   EN: {translated}")
    print()

print(f"â±ï¸  ë²ˆì—­ ì‹œê°„: {translate_time_llm_ko:.2f}ì´ˆ")
print()

# ===== Seq2Seq ëª¨ë¸ í…ŒìŠ¤íŠ¸ =====
print("=" * 70)
print("2. Seq2Seq ëª¨ë¸ (NLLB)")
print("=" * 70)
print()

# ì˜ì–´ â†’ í•œêµ­ì–´ (Seq2Seq)
print("ğŸ”„ Seq2Seq ëª¨ë¸ ë¡œë”© ì¤‘ (en2ko)...")
start_load = time.time()

model_en2ko = AutoModelForSeq2SeqLM.from_pretrained("NHNDQ/nllb-finetuned-en2ko", cache_dir=models_dir)
tokenizer_en2ko = AutoTokenizer.from_pretrained(
    "NHNDQ/nllb-finetuned-en2ko",
    cache_dir=models_dir,
    src_lang="eng_Latn",
    tgt_lang="kor_Hang"
)

if torch.cuda.is_available():
    model_en2ko = model_en2ko.cuda()

load_time_s2s_en = time.time() - start_load
print(f"âœ… Seq2Seq ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {load_time_s2s_en:.2f}ì´ˆ)")
print()

print("ğŸ“ ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ (Seq2Seq)")
start_translate = time.time()

s2s_results_en = []
for i, text in enumerate(test_cases_en, 1):
    inputs = tokenizer_en2ko(text, return_tensors="pt", padding=True)
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}

    outputs = model_en2ko.generate(**inputs, max_length=512)
    translated = tokenizer_en2ko.decode(outputs[0], skip_special_tokens=True)
    s2s_results_en.append(translated)

    print(f"{i}. EN: {text}")
    print(f"   KO: {translated}")
    print()

translate_time_s2s_en = time.time() - start_translate
print(f"â±ï¸  ë²ˆì—­ ì‹œê°„: {translate_time_s2s_en:.2f}ì´ˆ")
print()

# í•œêµ­ì–´ â†’ ì˜ì–´ (Seq2Seq)
print("ğŸ”„ Seq2Seq ëª¨ë¸ ë¡œë”© ì¤‘ (ko2en)...")
start_load = time.time()

model_ko2en = AutoModelForSeq2SeqLM.from_pretrained("NHNDQ/nllb-finetuned-ko2en", cache_dir=models_dir)
tokenizer_ko2en = AutoTokenizer.from_pretrained(
    "NHNDQ/nllb-finetuned-ko2en",
    cache_dir=models_dir,
    src_lang="kor_Hang",
    tgt_lang="eng_Latn"
)

if torch.cuda.is_available():
    model_ko2en = model_ko2en.cuda()

load_time_s2s_ko = time.time() - start_load
print(f"âœ… Seq2Seq ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {load_time_s2s_ko:.2f}ì´ˆ)")
print()

print("ğŸ“ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ (Seq2Seq)")
start_translate = time.time()

s2s_results_ko = []
for i, text in enumerate(test_cases_ko, 1):
    inputs = tokenizer_ko2en(text, return_tensors="pt", padding=True)
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}

    outputs = model_ko2en.generate(**inputs, max_length=512)
    translated = tokenizer_ko2en.decode(outputs[0], skip_special_tokens=True)
    s2s_results_ko.append(translated)

    print(f"{i}. KO: {text}")
    print(f"   EN: {translated}")
    print()

translate_time_s2s_ko = time.time() - start_translate
print(f"â±ï¸  ë²ˆì—­ ì‹œê°„: {translate_time_s2s_ko:.2f}ì´ˆ")
print()

# ===== ë¹„êµ ë¶„ì„ =====
print("=" * 70)
print("3. ë¹„êµ ë¶„ì„")
print("=" * 70)
print()

print("ğŸ“Š ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ ë¹„êµ")
print("-" * 70)
for i, (en_text, llm_text, s2s_text) in enumerate(zip(test_cases_en, llm_results_en, s2s_results_en), 1):
    print(f"{i}. ì›ë¬¸: {en_text}")
    print(f"   LLM:     {llm_text}")
    print(f"   Seq2Seq: {s2s_text}")
    print()

print("ğŸ“Š í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ ë¹„êµ")
print("-" * 70)
for i, (ko_text, llm_text, s2s_text) in enumerate(zip(test_cases_ko, llm_results_ko, s2s_results_ko), 1):
    print(f"{i}. ì›ë¬¸: {ko_text}")
    print(f"   LLM:     {llm_text}")
    print(f"   Seq2Seq: {s2s_text}")
    print()

# ===== ì„±ëŠ¥ ë¹„êµ =====
print("=" * 70)
print("4. ì„±ëŠ¥ ë¹„êµ")
print("=" * 70)
print()

print("â±ï¸  ë²ˆì—­ ì†ë„ ë¹„êµ:")
print()
print(f"ì˜ì–´ â†’ í•œêµ­ì–´:")
print(f"  LLM:     {translate_time_llm_en:.2f}ì´ˆ")
print(f"  Seq2Seq: {translate_time_s2s_en:.2f}ì´ˆ")
print(f"  ì†ë„ ë¹„ìœ¨: {translate_time_s2s_en/translate_time_llm_en:.2f}x (LLM ê¸°ì¤€)")
print()

print(f"í•œêµ­ì–´ â†’ ì˜ì–´:")
print(f"  LLM:     {translate_time_llm_ko:.2f}ì´ˆ")
print(f"  Seq2Seq: {translate_time_s2s_ko:.2f}ì´ˆ")
print(f"  ì†ë„ ë¹„ìœ¨: {translate_time_s2s_ko/translate_time_llm_ko:.2f}x (LLM ê¸°ì¤€)")
print()

print("ğŸ“¦ ëª¨ë¸ í¬ê¸°:")
print(f"  LLM:     7B íŒŒë¼ë¯¸í„°")
print(f"  Seq2Seq: 600M íŒŒë¼ë¯¸í„°")
print()

print("ğŸ’¾ VRAM ìš”êµ¬ì‚¬í•­:")
print(f"  LLM:     12-16GB")
print(f"  Seq2Seq: 3-4GB")
print()

# ===== ê²°ë¡  =====
print("=" * 70)
print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 70)
print()
print("ğŸ“ ê²°ë¡ :")
print()
print("LLM (iris-7b):")
print("  âœ… ë¹ ë¥¸ ë°°ì¹˜ ì²˜ë¦¬ (vLLM)")
print("  âœ… ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)")
print("  âœ… ê¸´ ë¬¸ì¥ ì™„ì „ ë²ˆì—­ (max_tokens=128)")
print("  âš ï¸  ë†’ì€ VRAM ìš”êµ¬ (12-16GB)")
print()
print("Seq2Seq (NLLB):")
print("  âœ… ì•ˆì •ì ì´ê³  ì •í™•í•œ ë²ˆì—­")
print("  âœ… ë‚®ì€ VRAM ìš”êµ¬ (3-4GB)")
print("  âš ï¸  ëŠë¦° ìˆœì°¨ ì²˜ë¦¬")
print()
print("ë‹¤ìŒ ë‹¨ê³„:")
print("  1. ë²ˆì—­ í’ˆì§ˆ ì§ì ‘ ë¹„êµ ë° í‰ê°€")
print("  2. ì‹¤ì œ íŒŒì¼(.txt, .epub, .srt)ë¡œ í…ŒìŠ¤íŠ¸")
print("  3. ì‚¬ìš© ì‚¬ë¡€ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ")
