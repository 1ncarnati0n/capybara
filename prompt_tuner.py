#!/usr/bin/env python3
"""
í”„ë¡¬í”„íŠ¸ íŠœë„ˆ v2.0 (Advanced Prompt Tuner)

ë‹¤ì–‘í•œ ì˜ì–´ í‘œí˜„(ê¸°ìˆ  ì „ë¬¸ ë¶„ì•¼, ë¬¸í•™ í‘œí˜„, ì‹ ì¡°ì–´, ì¼ìƒ, í•™ìˆ  ë“±)ì„
í…ŒìŠ¤íŠ¸í•˜ì—¬ iris-7b ëª¨ë¸ì˜ ìµœì  í”„ë¡¬í”„íŠ¸ ì „ëµì„ ì°¾ìŠµë‹ˆë‹¤.

íŠ¹ì§•:
- íŒŒì¼ ë‚´ë¶€ì— ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë‚´ì¥
- 5ê°œ ë„ë©”ì¸ ì§€ì›: ê¸°ìˆ , ë¬¸í•™, ì‹ ì¡°ì–´/ì†ì–´, ì¼ìƒ, í•™ìˆ 
- ë„ë©”ì¸ë³„ í‰ê°€ ì§€í‘œ (ì „ë¬¸ìš©ì–´ ë³´ì¡´, ë¬¸ì²´ ì¼ê´€ì„± ë“±)
- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµ í…ŒìŠ¤íŠ¸
- ê²°ê³¼ëŠ” capybara/ë¶„ì„ê²°ê³¼/*.md ë¡œ ì €ì¥

ì‚¬ìš© ì˜ˆì‹œ:
  conda activate capybara && cd capybara
  python test_prompt_tuner.py --domains all --limit 10
  python test_prompt_tuner.py --domains tech literary --limit 5
"""

import os
import sys
import time
import argparse
from datetime import datetime
from typing import List, Tuple, Dict
from collections import defaultdict

from vllm import LLM, SamplingParams
from langdetect import detect


# =============================================================================
# ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë°ì´í„°ì…‹
# =============================================================================

TEST_SENTENCES = {
    "tech": [
        # ê¸°ìˆ  ì „ë¬¸ ìš©ì–´ ë° ê°œë°œ í‘œí˜„
        "The microservice architecture enables horizontal scaling through containerization.",
        "We implemented a zero-trust security model using JWT tokens and OAuth2 flows.",
        "The GPU utilizes CUDA cores for parallel processing of tensor operations.",
        "Kubernetes orchestrates containerized applications across distributed clusters.",
        "The API gateway handles rate limiting, authentication, and request routing.",
        "We optimized database queries using indexing and query plan analysis.",
        "The CI/CD pipeline automates testing, building, and deployment processes.",
        "TensorFlow provides automatic differentiation for gradient-based optimization.",
        "The blockchain uses Merkle trees to ensure data integrity and immutability.",
        "WebAssembly enables near-native performance for web applications.",
        "The neural network employs dropout regularization to prevent overfitting.",
        "Redis implements an in-memory key-value store with persistence options.",
        "The distributed system achieves consensus through the Raft algorithm.",
        "GraphQL allows clients to request exactly the data they need.",
        "We leverage edge computing to reduce latency for real-time applications.",
    ],

    "literary": [
        # ë¬¸í•™ì  í‘œí˜„ ë° ë¹„ìœ ì  ì–¸ì–´
        "The autumn leaves danced gracefully in the gentle breeze.",
        "Her laughter echoed through the empty corridors like a forgotten melody.",
        "Time stood still as he gazed into her eyes, lost in their depths.",
        "The old house creaked and groaned, whispering secrets of bygone eras.",
        "His heart was a fortress, impenetrable and cold.",
        "The city slept beneath a blanket of stars, peaceful and serene.",
        "She carried the weight of the world on her delicate shoulders.",
        "The river of time flows ceaselessly, washing away all sorrows.",
        "His words were daggers, sharp and unforgiving.",
        "The moon hung low, a silver lantern in the velvet night.",
        "Memory is a treacherous companion, beautiful yet unreliable.",
        "The silence between them spoke volumes that words never could.",
        "Hope flickered like a candle flame in the darkness of despair.",
        "The ocean's whisper carried tales of distant lands and forgotten dreams.",
        "She was a wildflower, thriving in the most unexpected places.",
    ],

    "slang": [
        # ì‹ ì¡°ì–´, ì†ì–´, ì¸í„°ë„· ë°ˆ
        "That new feature is totally fire, no cap!",
        "He's ghosting me again, ugh, so annoying.",
        "Let's vibe check this project before we ship it.",
        "She's lowkey the GOAT at coding.",
        "This bug is sus, we need to investigate ASAP.",
        "I'm deadass tired of debugging this legacy code.",
        "That's a big yikes from me, chief.",
        "This API is chef's kiss, absolutely perfect.",
        "He really said that? The audacity!",
        "I'm gonna touch grass after this sprint.",
        "That solution is so galaxy brain, I love it.",
        "Stop the cap, you know that's not true.",
        "This meeting could've been an email, not gonna lie.",
        "She's giving main character energy today.",
        "Let's circle back on this after lunch, bet.",
    ],

    "casual": [
        # ì¼ìƒ íšŒí™” ë° ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
        "How's your day going so far?",
        "I'll grab some coffee on my way to the office.",
        "Let me know if you need any help with that.",
        "Thanks for reaching out, I really appreciate it.",
        "Sorry I'm running a bit late, traffic is crazy today.",
        "Would you like to grab lunch together sometime this week?",
        "I'm not sure I understand what you mean, could you explain?",
        "That sounds like a great idea, let's do it!",
        "I'm feeling a bit under the weather today.",
        "It's been a while since we last caught up.",
        "I'm swamped with work right now, can we talk later?",
        "Don't worry about it, these things happen.",
        "I'm looking forward to seeing you soon.",
        "Could you do me a favor and send that file over?",
        "I completely forgot about our meeting, my bad!",
    ],

    "academic": [
        # í•™ìˆ  ë…¼ë¬¸ ìŠ¤íƒ€ì¼ ë° formal writing
        "The empirical results demonstrate a statistically significant correlation.",
        "We propose a novel framework for evaluating neural network robustness.",
        "The methodology employed in this study builds upon prior research.",
        "Subsequent analysis revealed several noteworthy limitations.",
        "This phenomenon can be attributed to underlying systemic factors.",
        "The findings suggest a paradigm shift in our understanding of cognition.",
        "We hypothesize that increased exposure leads to enhanced performance.",
        "The data was collected through a double-blind randomized control trial.",
        "These results are consistent with previously established theories.",
        "Further investigation is warranted to elucidate the causal mechanisms.",
        "The study's implications extend beyond the immediate scope of inquiry.",
        "We observed a pronounced effect in the treatment group relative to controls.",
        "This approach mitigates potential confounding variables effectively.",
        "The theoretical framework provides a comprehensive lens for analysis.",
        "Our conclusions are subject to the inherent limitations of cross-sectional data.",
    ],
}


# =============================================================================
# í”„ë¡¬í”„íŠ¸ ì „ëµ ì •ì˜
# =============================================================================

def build_prompts(texts: List[str], direction: str, strategy: str, domain: str = "general") -> List[str]:
    """
    ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµ ìƒì„±

    Args:
        texts: ë²ˆì—­í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        direction: "en2ko" ë˜ëŠ” "ko2en"
        strategy: í”„ë¡¬í”„íŠ¸ ì „ëµëª…
        domain: ë„ë©”ì¸ íƒ€ì… (tech, literary, slang, casual, academic)
    """
    prompts: List[str] = []

    if direction not in ("en2ko", "ko2en"):
        raise ValueError("direction must be en2ko or ko2en")

    # ë„ë©”ì¸ë³„ ì§€ì¹¨
    domain_instructions = {
        "tech": "Preserve all technical terms and acronyms. Maintain precise technical meaning.",
        "literary": "Preserve literary tone, metaphors, and poetic expressions. Maintain emotional nuance.",
        "slang": "Translate slang naturally to equivalent Korean expressions. Preserve casual tone.",
        "casual": "Use natural, conversational Korean. Maintain friendly and approachable tone.",
        "academic": "Use formal academic Korean. Preserve scholarly precision and objectivity.",
        "general": "Preserve tone and nuances.",
    }

    domain_inst = domain_instructions.get(domain, domain_instructions["general"])

    # Strategy 1: Domain-Aware Few-Shot
    if strategy == "domain_fewshot":
        if direction == "en2ko":
            # ë„ë©”ì¸ë³„ ì˜ˆì‹œ ì„ íƒ
            if domain == "tech":
                examples = """Examples:
English: "The API returns a JSON response."
Korean: "APIëŠ” JSON ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."

English: "We use Docker for containerization."
Korean: "ì»¨í…Œì´ë„ˆí™”ë¥¼ ìœ„í•´ Dockerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
"""
            elif domain == "literary":
                examples = """Examples:
English: "Her eyes sparkled like stars."
Korean: "ê·¸ë…€ì˜ ëˆˆì€ ë³„ì²˜ëŸ¼ ë¹›ë‚¬ë‹¤."

English: "Time heals all wounds."
Korean: "ì‹œê°„ì€ ëª¨ë“  ìƒì²˜ë¥¼ ì¹˜ìœ í•œë‹¤."
"""
            else:
                examples = """Examples:
English: "Good morning."
Korean: "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤."

English: "Thank you very much."
Korean: "ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤."
"""

            template = f"""You are a professional translator specializing in {domain} content.
{domain_inst}
Output only the Korean translation without quotes or labels.

{examples}
Now translate this:
English: "{{text}}"
Korean:"""

        else:  # ko2en
            template = f"""You are a professional translator specializing in {domain} content.
{domain_inst}
Output only the English translation without quotes or labels.

Now translate this:
Korean: "{{text}}"
English:"""

        prompts = [template.format(text=t) for t in texts]

    # Strategy 2: Instruction-Only (No Examples)
    elif strategy == "instruct_only":
        if direction == "en2ko":
            template = f"""Translate English to Korean. {domain_inst}
Output only the translation, no quotes or labels.

Text: {{text}}
Translation:"""
        else:
            template = f"""Translate Korean to English. {domain_inst}
Output only the translation, no quotes or labels.

Text: {{text}}
Translation:"""

        prompts = [template.format(text=t) for t in texts]

    # Strategy 3: Minimal Prompt
    elif strategy == "minimal":
        if direction == "en2ko":
            template = "Translate to Korean:\n{text}\n"
        else:
            template = "Translate to English:\n{text}\n"

        prompts = [template.format(text=t) for t in texts]

    # Strategy 4: Chain-of-Thought Style
    elif strategy == "cot_style":
        if direction == "en2ko":
            template = f"""You are translating {domain} content from English to Korean.
{domain_inst}

Step 1: Understand the meaning and context
Step 2: Translate naturally to Korean
Step 3: Output ONLY the final Korean translation

English: {{text}}
Korean translation:"""
        else:
            template = f"""You are translating {domain} content from Korean to English.
{domain_inst}

Step 1: Understand the meaning and context
Step 2: Translate naturally to English
Step 3: Output ONLY the final English translation

Korean: {{text}}
English translation:"""

        prompts = [template.format(text=t) for t in texts]

    # Strategy 5: Role-Based Prompt
    elif strategy == "role_based":
        if direction == "en2ko":
            template = f"""You are an expert translator specialized in {domain} texts.
Your task: Translate English to natural Korean.
Requirements: {domain_inst}
Output: Only the Korean translation, nothing else.

[English Text]
{{text}}

[Korean Translation]
"""
        else:
            template = f"""You are an expert translator specialized in {domain} texts.
Your task: Translate Korean to natural English.
Requirements: {domain_inst}
Output: Only the English translation, nothing else.

[Korean Text]
{{text}}

[English Translation]
"""

        prompts = [template.format(text=t) for t in texts]

    else:
        raise ValueError(f"Unknown strategy: {strategy}")

    return prompts


# =============================================================================
# í‰ê°€ í•¨ìˆ˜
# =============================================================================

def score_output(inp: str, out: str, direction: str, domain: str) -> Tuple[float, Dict[str, float]]:
    """
    ë²ˆì—­ ì¶œë ¥ í‰ê°€ (ë„ë©”ì¸ë³„ íŠ¹í™” ì§€í‘œ í¬í•¨)

    Returns:
        (ì ìˆ˜, íŒ¨ë„í‹° ì„¸ë¶€ì‚¬í•­) íŠœí”Œ
    """
    score = 1.0
    penalties: Dict[str, float] = {}

    # 1. ì–¸ì–´ ê°ì§€
    target = "ko" if direction == "en2ko" else "en"
    try:
        out_lang = detect(out[:200]) if out.strip() else ""
    except Exception:
        out_lang = ""

    if target not in (out_lang or ""):
        score -= 0.5
        penalties["lang_mismatch"] = 0.5

    # 2. ê³¼ë„í•œ ê¸¸ì´
    if len(inp) > 0:
        ratio = len(out) / max(1, len(inp))
        if ratio > 3.5:
            score -= 0.3
            penalties["too_long_x3.5"] = 0.3
        elif ratio > 2.5:
            score -= 0.2
            penalties["too_long_x2.5"] = 0.2

    # 3. ë¶ˆí•„ìš”í•œ ë ˆì´ë¸”/ë§ˆì»¤
    noisy_markers = [
        "English:", "Korean:", "ë²ˆì—­:", "Translation:",
        "Here is", "This is", "[", "]", "Output:", "Result:"
    ]
    if any(m in out for m in noisy_markers):
        score -= 0.25
        penalties["noisy_markers"] = 0.25

    # 4. í°ë”°ì˜´í‘œ ê³¼ë‹¤ ì‚¬ìš©
    if out.strip().startswith(('"', "'")) and out.strip().endswith(('"', "'")):
        score -= 0.1
        penalties["quotes"] = 0.1

    # 5. ì¤„ë°”ê¿ˆ ê³¼ë‹¤
    if out.count("\n") >= 2:
        score -= 0.1
        penalties["excess_newlines"] = 0.1

    # 6. ë„ë©”ì¸ë³„ íŠ¹í™” í‰ê°€
    if domain == "tech":
        # ê¸°ìˆ  ìš©ì–´: ëŒ€ë¬¸ì ì•½ì–´ ë³´ì¡´ í™•ì¸
        tech_terms = ["API", "GPU", "CPU", "JSON", "HTTP", "SQL", "URL", "JWT", "OAuth", "CI", "CD"]
        input_terms = [t for t in tech_terms if t in inp]
        # í•œê¸€ ë²ˆì—­ì—ì„œ ì›ì–´ ë³´ì¡´ ë˜ëŠ” ì ì ˆí•œ ë²ˆì—­ ì—¬ë¶€ (ê°„ì ‘ ì²´í¬)
        if direction == "en2ko" and input_terms:
            # ì¶œë ¥ì— ê¸°ìˆ ìš©ì–´ê°€ ì¼ë¶€ë¼ë„ ë³´ì¡´ë˜ì—ˆëŠ”ì§€ í™•ì¸
            preserved = any(term in out for term in input_terms)
            if not preserved and len(input_terms) > 0:
                score -= 0.15
                penalties["tech_term_loss"] = 0.15

    elif domain == "literary":
        # ë¬¸í•™ í‘œí˜„: ê³¼ë„í•˜ê²Œ ì§ì—­ë˜ì§€ ì•Šì•˜ëŠ”ì§€ (ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
        if direction == "en2ko" and len(out) < len(inp) * 0.5:
            score -= 0.1
            penalties["literary_too_short"] = 0.1

    elif domain == "slang":
        # ì‹ ì¡°ì–´/ì†ì–´: ë„ˆë¬´ í˜•ì‹ì ì´ë©´ ì•ˆ ë¨ (ì¡´ëŒ“ë§ ê³¼ë‹¤ ì‚¬ìš© ì²´í¬)
        if direction == "en2ko":
            formal_markers = ["ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì‹­ì‹œì˜¤"]
            if sum(out.count(m) for m in formal_markers) > 2:
                score -= 0.1
                penalties["slang_too_formal"] = 0.1

    elif domain == "academic":
        # í•™ìˆ  í‘œí˜„: ë„ˆë¬´ êµ¬ì–´ì²´ë©´ ì•ˆ ë¨
        if direction == "en2ko":
            casual_markers = ["ã…‹", "ã…", "!", "~", "ìš”~"]
            if any(m in out for m in casual_markers):
                score -= 0.15
                penalties["academic_too_casual"] = 0.15

    return max(0.0, score), penalties


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def setup_logging(prefix: str = "prompt_tuner_v2") -> str:
    """ë¡œê¹… íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    base_dir = os.path.dirname(__file__)
    results_dir = os.path.join(base_dir, "ë¶„ì„ê²°ê³¼")
    os.makedirs(results_dir, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = os.path.join(results_dir, f"{prefix}_{ts}.log")
    return path


def chunk(lst: List[str], size: int) -> List[List[str]]:
    """ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• """
    return [lst[i:i + size] for i in range(0, len(lst), size)]


def detect_direction(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€"""
    lang = "en"
    try:
        lang = detect(text[:200])
    except Exception:
        pass
    return "en2ko" if lang.startswith("en") else "ko2en"


# =============================================================================
# ë©”ì¸ í•¨ìˆ˜
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="í”„ë¡¬í”„íŠ¸ íŠœë„ˆ v2.0 - ë„ë©”ì¸ë³„ ë²ˆì—­ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"
    )
    parser.add_argument(
        "--domains",
        nargs="+",
        choices=["tech", "literary", "slang", "casual", "academic", "all"],
        default=["all"],
        help="í…ŒìŠ¤íŠ¸í•  ë„ë©”ì¸ ì„ íƒ (ê¸°ë³¸ê°’: all)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="ê° ë„ë©”ì¸ë‹¹ í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )
    parser.add_argument(
        "--model",
        default="davidkim205/iris-7b",
        help="ì‚¬ìš©í•  ëª¨ë¸ëª…"
    )
    parser.add_argument(
        "--max_model_len",
        type=int,
        default=1024,
        help="ìµœëŒ€ ëª¨ë¸ ê¸¸ì´"
    )
    parser.add_argument(
        "--gpu_mem",
        type=float,
        default=0.91,
        help="GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (0.0~1.0)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.2,
        help="ìƒ˜í”Œë§ temperature (ê¸°ë³¸ê°’: 0.2)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16)"
    )

    args = parser.parse_args()

    # ë„ë©”ì¸ ì„ íƒ
    if "all" in args.domains:
        selected_domains = list(TEST_SENTENCES.keys())
    else:
        selected_domains = args.domains

    # ë¡œê¹… íŒŒì¼ ì¤€ë¹„
    out_path = setup_logging()

    # ëª¨ë¸ ë¡œë”©
    print("=" * 70)
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {args.model}")
    print("=" * 70)

    models_dir = os.path.join(os.getcwd(), "models")
    os.makedirs(models_dir, exist_ok=True)

    llm = LLM(
        model=args.model,
        download_dir=models_dir,
        tensor_parallel_size=1,
        gpu_memory_utilization=args.gpu_mem,
        max_model_len=args.max_model_len,
        dtype="auto",
        kv_cache_dtype="fp8",
        enforce_eager=True,
        trust_remote_code=True,
    )

    sampling = SamplingParams(
        temperature=args.temperature,
        top_p=0.95,
        max_tokens=256,
        repetition_penalty=1.1,
        skip_special_tokens=True,
        stop=["\n\n", "English:", "Korean:", "---", "###", "[END]"],
    )

    # í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ ì „ëµë“¤
    strategies = [
        "domain_fewshot",
        "instruct_only",
        "minimal",
        "cot_style",
        "role_based",
    ]

    # ê²°ê³¼ ì €ì¥ìš©
    all_results = defaultdict(lambda: defaultdict(dict))

    # ë¡œê·¸ íŒŒì¼ ì‘ì„±
    with open(out_path, "w", encoding="utf-8") as wf:
        # í—¤ë”
        wf.write("[LOG] ê²°ê³¼ íŒŒì¼: {}\n".format(out_path))
        wf.write("="*70 + "\n")
        wf.write("í”„ë¡¬í”„íŠ¸ íŠœë„ˆ v2.0 - ë„ë©”ì¸ë³„ ë²ˆì—­ í’ˆì§ˆ í…ŒìŠ¤íŠ¸\n")
        wf.write("="*70 + "\n\n")
        wf.write(f"ğŸ”§ ëª¨ë¸: {args.model}\n")
        wf.write(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë„ë©”ì¸: {', '.join(selected_domains)}\n")
        wf.write(f"ğŸ“ ê° ë„ë©”ì¸ë‹¹ ë¬¸ì¥ ìˆ˜: {args.limit}\n")
        wf.write(f"âš™ï¸  íŒŒë¼ë¯¸í„°: gpu_mem={args.gpu_mem}, max_model_len={args.max_model_len}, temperature={args.temperature}\n")
        wf.write(f"ğŸ• ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸
        for domain in selected_domains:
            print(f"\n{'='*70}")
            print(f"ë„ë©”ì¸: {domain.upper()}")
            print(f"{'='*70}")

            wf.write("="*70 + "\n")
            wf.write(f"ë„ë©”ì¸: {domain.upper()}\n")
            wf.write("="*70 + "\n\n")

            samples = TEST_SENTENCES[domain][:args.limit]
            direction = "en2ko"  # ì˜í•œ ë²ˆì—­ ê³ ì • (í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì˜ì–´ì´ë¯€ë¡œ)

            wf.write(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ {len(samples)}ê°œ:\n")
            for i, s in enumerate(samples, 1):
                wf.write(f"  {i}. {s}\n")
            wf.write("\n")

            # ê° ì „ëµë³„ í…ŒìŠ¤íŠ¸
            for strat in strategies:
                print(f"\nì „ëµ: {strat}")

                total_score = 0.0
                n = 0
                detail_rows = []

                # ë°°ì¹˜ ì¶”ë¡ 
                for batch in chunk(samples, args.batch_size):
                    prompts = build_prompts(batch, direction, strat, domain)
                    outputs = llm.generate(prompts, sampling)

                    for inp, out in zip(batch, outputs):
                        text = out.outputs[0].text.strip().strip('"').strip("'").strip()

                        # ë„ë©”ì¸ë³„ í‰ê°€
                        s, penalties = score_output(inp, text, direction, domain)
                        total_score += s
                        n += 1
                        detail_rows.append((inp, text, s, penalties))

                avg = total_score / max(1, n)
                all_results[domain][strat] = {
                    "avg_score": avg,
                    "details": detail_rows
                }

                print(f"  í‰ê·  ì ìˆ˜: {avg:.3f}")

                wf.write(f"ğŸ”¹ ì „ëµ: {strat}\n")
                wf.write(f"   í‰ê·  ì ìˆ˜: {avg:.3f}\n")
                wf.write(f"   ìƒ˜í”Œ 3ê°œ ì˜ˆì‹œ:\n\n")
                for i, (inp, out, s, pen) in enumerate(detail_rows[:3], 1):
                    wf.write(f"   [ì˜ˆì‹œ {i}]\n")
                    wf.write(f"   ì›ë¬¸: {inp}\n")
                    wf.write(f"   ë²ˆì—­: {out}\n")
                    wf.write(f"   ì ìˆ˜: {s:.3f}\n")
                    if pen:
                        wf.write(f"   íŒ¨ë„í‹°: {pen}\n")
                    wf.write("\n")

            wf.write("\n")

        # ì „ì²´ ìš”ì•½
        wf.write("="*70 + "\n")
        wf.write("ì „ì²´ ìš”ì•½\n")
        wf.write("="*70 + "\n\n")

        wf.write("ğŸ“Š ë„ë©”ì¸ë³„ ìµœê³  ì „ëµ\n")
        wf.write("-"*70 + "\n\n")

        for domain in selected_domains:
            best_strat = max(
                all_results[domain].items(),
                key=lambda x: x[1]["avg_score"]
            )
            strat_name, strat_data = best_strat

            wf.write(f"ğŸ† {domain.upper()}\n")
            wf.write(f"   ìµœê³  ì „ëµ: {strat_name}\n")
            wf.write(f"   í‰ê·  ì ìˆ˜: {strat_data['avg_score']:.3f}\n\n")

        # ì „ëµë³„ í‰ê·  ì ìˆ˜
        wf.write("ğŸ“ˆ ì „ëµë³„ ì „ì²´ í‰ê·  ì ìˆ˜\n")
        wf.write("-"*70 + "\n\n")

        strategy_totals = defaultdict(list)
        for domain in selected_domains:
            for strat in strategies:
                strategy_totals[strat].append(all_results[domain][strat]["avg_score"])

        strategy_avgs = {
            strat: sum(scores) / len(scores)
            for strat, scores in strategy_totals.items()
        }

        sorted_strategies = sorted(
            strategy_avgs.items(),
            key=lambda x: x[1],
            reverse=True
        )

        for rank, (strat, avg) in enumerate(sorted_strategies, 1):
            wf.write(f"  {rank}. {strat}: {avg:.3f}\n")

        # ìµœì¢… ì¶”ì²œ
        wf.write("\n" + "="*70 + "\n")
        wf.write("ìµœì¢… ì¶”ì²œ\n")
        wf.write("="*70 + "\n\n")

        best_overall = sorted_strategies[0]
        wf.write(f"âœ… ì „ì²´ ìµœê³  ì „ëµ: {best_overall[0]}\n")
        wf.write(f"   í‰ê·  ì ìˆ˜: {best_overall[1]:.3f}\n\n")

        wf.write("ğŸ’¡ ê¶Œì¥ì‚¬í•­\n")
        wf.write("-"*70 + "\n")
        wf.write("  1. ë²”ìš© ë²ˆì—­ì—ëŠ” '{}' ì „ëµì„ ì‚¬ìš©í•˜ì„¸ìš”.\n".format(best_overall[0]))
        wf.write("  2. íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš°, í•´ë‹¹ ë„ë©”ì¸ì˜ ìµœê³  ì „ëµì„ ì°¸ê³ í•˜ì„¸ìš”.\n")
        wf.write("  3. ê¸°ìˆ  ë¬¸ì„œëŠ” few-shot ì˜ˆì‹œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.\n")
        wf.write("  4. ë¬¸í•™ ì‘í’ˆì€ ë¬¸ì²´ ë³´ì¡´ì„ ëª…ì‹œí•œ instructionì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n")
        wf.write("  5. êµ¬ì–´ì²´/ì‹ ì¡°ì–´ëŠ” í˜•ì‹ì  í‘œí˜„ì„ í”¼í•˜ë„ë¡ ì§€ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n")

        wf.write("\n")

    print("\n" + "=" * 70)
    print("í”„ë¡¬í”„íŠ¸ íŠœë„ˆ v2.0 ì‹¤í–‰ ì™„ë£Œ!")
    print(f"ê²°ê³¼ íŒŒì¼: {out_path}")
    print("=" * 70)


if __name__ == "__main__":
    main()
